# Scout Portable Deployment Configuration
# Copy to .env and customize for your platform

# =============================================================================
# PLATFORM SELECTION
# =============================================================================
# Options: rpi, windows-gpu, linux-gpu, cpu-only
PLATFORM=cpu-only

# =============================================================================
# OLLAMA CONFIGURATION
# =============================================================================
# Where Ollama is running (use host.docker.internal for local Ollama on host)
OLLAMA_HOST=http://host.docker.internal:11434

# Model selection based on available resources:
#   RPi / CPU-only:  qwen2.5:3b (2GB VRAM/RAM)
#   8GB GPU:         qwen2.5:7b (5GB VRAM)
#   12GB+ GPU:       qwen2.5:14b (9GB VRAM)
#   24GB+ GPU:       qwen2.5:32b (20GB VRAM)
OLLAMA_MODEL=qwen2.5:3b

# Ollama port (if running via docker-compose)
OLLAMA_PORT=11434

# =============================================================================
# SCOUT CONFIGURATION
# =============================================================================
SCOUT_PORT=8000
SCOUT_LOG_LEVEL=INFO
SCOUT_ENVIRONMENT=production

# LLM timeout in seconds (increase for slow hardware)
SCOUT_LLM_TIMEOUT=300

# =============================================================================
# BENCHMARK CONFIGURATION
# =============================================================================
# Number of benchmark runs per test job
BENCHMARK_RUNS=3

# Scout URL for benchmark container
SCOUT_URL=http://localhost:8000
